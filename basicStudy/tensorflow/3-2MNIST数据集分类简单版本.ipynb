{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNST数据集\n",
    "\n",
    "来自http://yann.lecun.com/\n",
    "专门做手写数字识别的数据集，60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）  ；\n",
    "每一张图片包含了28*28个像素，像素点的用0-1间的值表示其明暗强度， 像素点为1表示为黑色，像素点为0表示为白色，然后是0.1到0.9依次接近黑色1。  \n",
    "所以可以把这些像素排成一行数组，展开成一个向量，长度为28*28=784.所以我们用TensorFlow的时候可以当做一个形状为[60000,784]的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每个图片中的像素点。\n",
    "\n",
    "数据集的分类标签是节约0-9的数字，我们可以将其one-hot编码为10个值得向量，如[0,0,1,0,0,0,0,0,0,0]表示2.\n",
    "因此mnist.train.labels是一个[60000,10]的数字矩阵。\n",
    "\n",
    "## 构建神经网络\n",
    "这里可以看出是有10个类别的多分类任务，建立简单的神经网络模型，输入层784个神经元，输出10个神经元。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](image/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax函数\n",
    "输出结果为10个神经元，每个神经元的值为其对应数字的概率，比如最终推测一张图片是9的概率为80%，并且10个点的概率和为1。  \n",
    "使用softmax函数模型可以用来给不同的对象分配概率。  \n",
    "$softmax(x)_i = \\frac{exp(x_i)}{\\sum^{j \\to 9}_{j = 1}{exp(x_j)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如输出结果为[1,5,3]:\n",
    "\n",
    "$e^1 = 2.718$ ,\n",
    "$e^5 = 148.413$,\n",
    "$e^3 = 20.086$;  \n",
    "$p1 = \\frac{e^1}{e^1+e^5+e^3} = 0.016$  \n",
    "$p2 = \\frac{e^5}{e^1+e^5+e^3} = 0.867$   \n",
    "$p3 = \\frac{e^3}{e^1+e^5+e^3} = 0.117$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 0,Testing Accuracy 0.8304\n",
      "Iter 1,Testing Accuracy 0.8716\n",
      "Iter 2,Testing Accuracy 0.882\n",
      "Iter 3,Testing Accuracy 0.889\n",
      "Iter 4,Testing Accuracy 0.8934\n",
      "Iter 5,Testing Accuracy 0.8962\n",
      "Iter 6,Testing Accuracy 0.8998\n",
      "Iter 7,Testing Accuracy 0.9016\n",
      "Iter 8,Testing Accuracy 0.9033\n",
      "Iter 9,Testing Accuracy 0.905\n",
      "Iter 10,Testing Accuracy 0.9063\n",
      "Iter 11,Testing Accuracy 0.907\n",
      "Iter 12,Testing Accuracy 0.9084\n",
      "Iter 13,Testing Accuracy 0.9093\n",
      "Iter 14,Testing Accuracy 0.9097\n",
      "Iter 15,Testing Accuracy 0.9114\n",
      "Iter 16,Testing Accuracy 0.9117\n",
      "Iter 17,Testing Accuracy 0.912\n",
      "Iter 18,Testing Accuracy 0.9129\n",
      "Iter 19,Testing Accuracy 0.9133\n",
      "Iter 20,Testing Accuracy 0.9143\n",
      "Iter 21,Testing Accuracy 0.915\n",
      "Iter 22,Testing Accuracy 0.9159\n",
      "Iter 23,Testing Accuracy 0.9153\n",
      "Iter 24,Testing Accuracy 0.9163\n",
      "Iter 25,Testing Accuracy 0.9164\n",
      "Iter 26,Testing Accuracy 0.9168\n",
      "Iter 27,Testing Accuracy 0.9173\n",
      "Iter 28,Testing Accuracy 0.9178\n",
      "Iter 29,Testing Accuracy 0.9178\n"
     ]
    }
   ],
   "source": [
    "#载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "#将训练数据分成几个批次导入训练\n",
    "#每个批次的大小，100个样本量\n",
    "batch_size = 100 \n",
    "#计算一共有多少个批次\n",
    "n_batch = mnist.train.num_examples // batch_size   #//为floor除，向下取整\n",
    "\n",
    "#定义两个placeholder\n",
    "x = tf.placeholder(tf.float32,[None,784])  ## Here None means that a dimension can be of any length\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "#创建一个简单的神经网络\n",
    "W = tf.Variable(tf.zeros([784,10]))  #权重，in_size*out_size\n",
    "b = tf.Variable(tf.zeros([10]))      #biases，out_size\n",
    "prediction = tf.nn.softmax(tf.matmul(x,W)+b)  #计算预测值 ，softmax  \n",
    "\n",
    "#二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "#使用梯度下降法\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "#初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#结果存放在一个布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))#argmax返回一维张量中最大的值所在的位置\n",
    "#求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(30):  #迭代次数\n",
    "        for batch in range(n_batch):   #批次数\n",
    "            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})  #训练\n",
    "        #计算准确率\n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
