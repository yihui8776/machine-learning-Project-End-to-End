{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:04:11.079713Z",
     "start_time": "2020-04-02T07:04:11.919Z"
    }
   },
   "source": [
    "## 背景\n",
    "这是来自kaggle的一个数据集，主要是来自一个面包店的两万多条交易记录。提供的数据可以用于商业分析，产品营销推荐等，没有task，一般是自己定义的问题。https://www.kaggle.com/sulmansarwar/transactions-from-a-bakery  \n",
    "\n",
    "每条记录有日期，时间、交易项等，交易项是一个商品，比如Tea、bread等，也就是表示每个商品被销售的时间。\n",
    "这里主要学习关联规则，参考https://www.kaggle.com/bbhatt001/bakery-business-model-association-rules \n",
    "\n",
    "这里主要测试pyspark 和 networkx\n",
    "ml是spark机器学习库类似mllib，不同的是ml是二次封装，基于DataFrame结构，更贴近数据科学\n",
    "MLlib是基于rdd，更贴近底层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:29:41.241199Z",
     "start_time": "2020-04-02T07:29:41.476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date,Time,Transaction,Item\n"
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "#创建上下文\n",
    "spark=SparkSession.builder.appName(\"FPgrowth\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "#读取数据\n",
    "lines = sc.textFile('input/BreadBasket_DMS.csv')\n",
    "#如果你的csv文件有标题 的话，需要剔除首行\n",
    "header = lines.first() \n",
    "\n",
    "#第一行 \n",
    "print(header)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:48:52.542536Z",
     "start_time": "2020-04-02T07:48:52.294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date=datetime.datetime(2016, 10, 30, 0, 0), Time='09:58:11', Transaction=1, Item='Bread')\n"
     ]
    }
   ],
   "source": [
    "#ml的fpgrowth\n",
    "\n",
    "import datetime\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t1=datetime.datetime.now()\n",
    "    #指定schema：\n",
    "    schema = StructType([\n",
    "        # true代表不为null\n",
    "        StructField(\"Date\", StringType(), True), # nullable=True, this field can not be null\n",
    "        StructField(\"Time\", StringType(), True),\n",
    "        StructField(\"Transaction\", StringType(), True),\n",
    "        StructField(\"Item\",StringType(),True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    #data = spark.read.csv(r\"hdfs://my_master:8020/user/root/data_spark.csv\", encoding='gbk', header=True, inferSchema=True) \n",
    "    # header表示数据的第一行是否为列名，inferSchema表示自动推断schema,未指定schema设为True\n",
    "    data = spark.read.csv(r\"input/BreadBasket_DMS.csv\",header=True,inferSchema=True)  #直接为DataFrame\n",
    "    \n",
    "    \n",
    "    print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:58:07.856190Z",
     "start_time": "2020-04-02T07:58:08.124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Transaction: integer (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#以树的形式打印概要\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:54:48.096862Z",
     "start_time": "2020-04-02T07:54:48.334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----------+-------------+\n",
      "|               Date|    Time|Transaction|         Item|\n",
      "+-------------------+--------+-----------+-------------+\n",
      "|2016-10-30 00:00:00|09:58:11|          1|        Bread|\n",
      "|2016-10-30 00:00:00|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30 00:00:00|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30 00:00:00|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30 00:00:00|10:07:57|          3|          Jam|\n",
      "|2016-10-30 00:00:00|10:07:57|          3|      Cookies|\n",
      "|2016-10-30 00:00:00|10:08:41|          4|       Muffin|\n",
      "|2016-10-30 00:00:00|10:13:03|          5|       Coffee|\n",
      "|2016-10-30 00:00:00|10:13:03|          5|       Pastry|\n",
      "|2016-10-30 00:00:00|10:13:03|          5|        Bread|\n",
      "|2016-10-30 00:00:00|10:16:55|          6|    Medialuna|\n",
      "|2016-10-30 00:00:00|10:16:55|          6|       Pastry|\n",
      "|2016-10-30 00:00:00|10:16:55|          6|       Muffin|\n",
      "|2016-10-30 00:00:00|10:19:12|          7|    Medialuna|\n",
      "|2016-10-30 00:00:00|10:19:12|          7|       Pastry|\n",
      "|2016-10-30 00:00:00|10:19:12|          7|       Coffee|\n",
      "|2016-10-30 00:00:00|10:19:12|          7|          Tea|\n",
      "|2016-10-30 00:00:00|10:20:51|          8|       Pastry|\n",
      "|2016-10-30 00:00:00|10:20:51|          8|        Bread|\n",
      "|2016-10-30 00:00:00|10:21:59|          9|        Bread|\n",
      "+-------------------+--------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------------+--------+-----------+-------------+\n",
      "|               Date|    Time|Transaction|         Item|\n",
      "+-------------------+--------+-----------+-------------+\n",
      "|2016-10-30 00:00:00|09:58:11|          1|        Bread|\n",
      "|2016-10-30 00:00:00|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30 00:00:00|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30 00:00:00|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30 00:00:00|10:07:57|          3|          Jam|\n",
      "|2016-10-30 00:00:00|10:07:57|          3|      Cookies|\n",
      "|2016-10-30 00:00:00|10:08:41|          4|       Muffin|\n",
      "|2016-10-30 00:00:00|10:13:03|          5|       Coffee|\n",
      "|2016-10-30 00:00:00|10:13:03|          5|       Pastry|\n",
      "|2016-10-30 00:00:00|10:13:03|          5|        Bread|\n",
      "+-------------------+--------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:56:27.663951Z",
     "start_time": "2020-04-02T07:56:26.510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#看有多少无效数据\n",
    "data[data['Item']=='NONE'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T08:02:53.244986Z",
     "start_time": "2020-04-02T08:02:53.876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20507"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#去掉空值\n",
    "data_drop = data[data['Item']!='NONE'] \n",
    "data_drop.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T08:04:02.511023Z",
     "start_time": "2020-04-02T08:04:02.418Z"
    }
   },
   "source": [
    "查看数据集里销售量排名前十的商品有那几个,并画条形图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:50:04.727003Z",
     "start_time": "2020-04-02T07:50:04.806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
       "    return f(*a, **kw)\n",
       "  File \"/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
       "    format(target_id, \".\", name), value)\n",
       "py4j.protocol.Py4JJavaError: An error occurred while calling o978.fit.\n",
       ": java.lang.IllegalArgumentException: Field \"items\" does not exist.\n",
       "Available fields: Date, Time, Transaction, Item\n",
       "\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n",
       "\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n",
       "\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n",
       "\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n",
       "\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n",
       "\tat org.apache.spark.ml.fpm.FPGrowthParams$class.validateAndTransformSchema(FPGrowth.scala:111)\n",
       "\tat org.apache.spark.ml.fpm.FPGrowth.validateAndTransformSchema(FPGrowth.scala:132)\n",
       "\tat org.apache.spark.ml.fpm.FPGrowth.transformSchema(FPGrowth.scala:200)\n",
       "\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n",
       "\tat org.apache.spark.ml.fpm.FPGrowth.fit(FPGrowth.scala:161)\n",
       "\tat org.apache.spark.ml.fpm.FPGrowth.fit(FPGrowth.scala:132)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-79624e9f-4246-446b-9dcc-2e94e1fe43f2/pyspark_runner.py\", line 194, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 3, in <module>\n",
       "  File \"/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
       "    return self._fit(dataset)\n",
       "  File \"/spark/python/pyspark/ml/wrapper.py\", line 295, in _fit\n",
       "    java_model = self._fit_java(dataset)\n",
       "  File \"/spark/python/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
       "    return self._java_obj.fit(dataset._jdf)\n",
       "  File \"/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
       "    answer, self.gateway_client, self.target_id, self.name)\n",
       "  File \"/spark/python/pyspark/sql/utils.py\", line 79, in deco\n",
       "    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\n",
       "pyspark.sql.utils.IllegalArgumentException: 'Field \"items\" does not exist.\\nAvailable fields: Date, Time, Transaction, Item'\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.GeneratedMethodAccessor169.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:282)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "#data=spark.createDataFrame(lines_df,[\"items\"])#将数据转为spark中的dataframe\n",
    "fp = FPGrowth(minSupport=0.5, minConfidence=0.8)#模型建立\n",
    "fpm  = fp.fit(data)#模型拟合\n",
    "fpm .freqItemsets.show(5)#在控制台显示前五条频繁项集\n",
    "assRule=fpm.associationRules#强关联规则\n",
    "assRuleDf=assRule.toPandas()#转为python中的dataframe  \n",
    "print('强关联规则：\\n',assRuleDf)\n",
    "new_data = spark.createDataFrame([([\"s\", \"t\"], )], [\"items\"])#新的前项数据\n",
    "print('后项预测：\\n',fpm.transform(new_data).first().prediction) #预测后项               \n",
    "spark.stop()#关闭spark\n",
    "t2=datetime.datetime.now()\n",
    "print('spent ts:',t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.7.7\n"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
