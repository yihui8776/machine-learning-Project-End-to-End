{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 复购建模改进版\n",
    "当时只做了baseline，后面再进一步做点改进\n",
    "\n",
    "###  数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:22:06.162425Z",
     "start_time": "2020-04-17T01:21:33.988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spark/python/pyspark/context.py:123: UserWarning: You are passing in an insecure Py4j gateway.  This presents a security risk, and will be completely forbidden in Spark 3.0\n",
      "  \"You are passing in an insecure Py4j gateway.  This \"\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "from pyspark.sql import SparkSession \n",
    "spark=SparkSession.builder.appName(\"dataframe\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "train = spark.read.csv(r\"data/train.csv\",header=True,inferSchema=True)\n",
    "test  = spark.read.csv(r\"data/test.csv\",header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:22:51.190300Z",
     "start_time": "2020-04-17T01:22:43.531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|   -1|6769859|\n",
      "|    1|  15952|\n",
      "|    0| 244912|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查看需要数据\n",
    "train.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:22:53.216411Z",
     "start_time": "2020-04-17T01:22:52.915Z"
    }
   },
   "outputs": [],
   "source": [
    "train1 = train.where(~(train['label']==-1))  #使用的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:23:11.796390Z",
     "start_time": "2020-04-17T01:22:54.946Z"
    }
   },
   "outputs": [],
   "source": [
    "# 一般100M内转为pandas的DataFrame做处理，减少内存使用\n",
    "train2 = train1.toPandas() \n",
    "test1 = test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:23:53.978594Z",
     "start_time": "2020-04-17T01:23:53.866Z"
    }
   },
   "outputs": [],
   "source": [
    "train2['age_range'].fillna('0',inplace=True)\n",
    "test1['age_range'].fillna('0',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:23:56.136656Z",
     "start_time": "2020-04-17T01:23:55.673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0    69369\n",
      "0.0    55809\n",
      "4.0    51235\n",
      "2.0    31026\n",
      "5.0    25618\n",
      "6.0    21701\n",
      "7.0     4120\n",
      "0       1253\n",
      "8.0      720\n",
      "1.0       13\n",
      "Name: age_range, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train2['gender'].fillna('2',inplace=True)\n",
    "test1['gender'].fillna('2',inplace=True)\n",
    "print(train2['age_range'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:23:57.319915Z",
     "start_time": "2020-04-17T01:23:57.370Z"
    }
   },
   "outputs": [],
   "source": [
    "train2['age_range'] = train2['age_range'].astype(int)\n",
    "train2['gender'] = train2['gender'].astype(int)\n",
    "test1['age_range'] = test1['age_range'].astype(int)\n",
    "test1['gender'] = test1['gender'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:23:59.337137Z",
     "start_time": "2020-04-17T01:23:59.194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    69369\n",
      "0    57062\n",
      "4    51235\n",
      "2    31026\n",
      "5    25618\n",
      "6    21701\n",
      "7     4120\n",
      "8      720\n",
      "1       13\n",
      "Name: age_range, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train2['age_range'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:24:02.356395Z",
     "start_time": "2020-04-17T01:24:01.914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    176414\n",
      "1     73756\n",
      "2     10694\n",
      "Name: gender, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train2['gender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:25:03.696862Z",
     "start_time": "2020-04-17T01:24:04.027Z"
    }
   },
   "outputs": [],
   "source": [
    "#转为spark的dataframe\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "spark_train = sqlContext.createDataFrame(train2)\n",
    "spark_test = sqlContext.createDataFrame(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:25:04.702551Z",
     "start_time": "2020-04-17T01:24:14.082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- age_range: long (nullable = true)\n",
      " |-- gender: long (nullable = true)\n",
      " |-- merchant_id: long (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:25:22.753459Z",
     "start_time": "2020-04-17T01:25:22.062Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "spark_train = spark_train.withColumn(\"user_id\", spark_train[\"user_id\"].cast(IntegerType()))\n",
    "spark_train = spark_train.withColumn(\"age_range\", spark_train[\"age_range\"].cast(IntegerType()))\n",
    "spark_train = spark_train.withColumn(\"gender\", spark_train[\"gender\"].cast(IntegerType()))\n",
    "spark_train = spark_train.withColumn(\"merchant_id\", spark_train[\"merchant_id\"].cast(IntegerType()))\n",
    "spark_train = spark_train.withColumn(\"label\", spark_train[\"label\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:25:25.768883Z",
     "start_time": "2020-04-17T01:25:25.738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'age_range', 'merchant_id']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#特征\n",
    "features = list(set(spark_train.columns)-set(['label','user_id']))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:25:30.332960Z",
     "start_time": "2020-04-17T01:25:27.468Z"
    }
   },
   "outputs": [],
   "source": [
    "#onehot\n",
    "from pyspark.ml.feature import OneHotEncoder \n",
    "from pyspark.ml import Pipeline\n",
    "##创建OneHotEncoder对象，设定输入输出参数\n",
    "categoryFeaturesIndex = ['age_range','merchant_id']\n",
    "pipeline = Pipeline(stages=[\n",
    "    OneHotEncoder(inputCol=c, outputCol='{}_vec'.format(c))\n",
    "    for c in categoryFeaturesIndex\n",
    "])\n",
    "\n",
    "onehot = pipeline.fit(spark_train)\n",
    "encodeData = onehot.transform(spark_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:25:31.598409Z",
     "start_time": "2020-04-17T01:25:30.411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-----------+-----+-------------+-------------------+\n",
      "|user_id|age_range|gender|merchant_id|label|age_range_vec|    merchant_id_vec|\n",
      "+-------+---------+------+-----------+-----+-------------+-------------------+\n",
      "|  34176|        6|     0|       3906|    0|(8,[6],[1.0])|(4993,[3906],[1.0])|\n",
      "|  34176|        6|     0|        121|    0|(8,[6],[1.0])| (4993,[121],[1.0])|\n",
      "|  34176|        6|     0|       4356|    1|(8,[6],[1.0])|(4993,[4356],[1.0])|\n",
      "|  34176|        6|     0|       2217|    0|(8,[6],[1.0])|(4993,[2217],[1.0])|\n",
      "| 230784|        0|     0|       4818|    0|(8,[0],[1.0])|(4993,[4818],[1.0])|\n",
      "| 362112|        4|     1|       2618|    0|(8,[4],[1.0])|(4993,[2618],[1.0])|\n",
      "|  34944|        5|     0|       2051|    0|(8,[5],[1.0])|(4993,[2051],[1.0])|\n",
      "| 231552|        5|     0|       3828|    1|(8,[5],[1.0])|(4993,[3828],[1.0])|\n",
      "| 231552|        5|     0|       2124|    0|(8,[5],[1.0])|(4993,[2124],[1.0])|\n",
      "| 232320|        4|     1|       1168|    0|(8,[4],[1.0])|(4993,[1168],[1.0])|\n",
      "| 232320|        4|     1|       4270|    0|(8,[4],[1.0])|(4993,[4270],[1.0])|\n",
      "| 167040|        5|     0|        671|    0|(8,[5],[1.0])| (4993,[671],[1.0])|\n",
      "| 101760|        0|     0|       1760|    0|(8,[0],[1.0])|(4993,[1760],[1.0])|\n",
      "| 298368|        4|     0|       2981|    0|(8,[4],[1.0])|(4993,[2981],[1.0])|\n",
      "|  36480|        6|     1|       4730|    0|(8,[6],[1.0])|(4993,[4730],[1.0])|\n",
      "| 299136|        0|     0|       2935|    0|(8,[0],[1.0])|(4993,[2935],[1.0])|\n",
      "|  37248|        3|     1|       2615|    0|(8,[3],[1.0])|(4993,[2615],[1.0])|\n",
      "| 103296|        6|     0|       2482|    0|(8,[6],[1.0])|(4993,[2482],[1.0])|\n",
      "| 299904|        6|     1|       1742|    0|(8,[6],[1.0])|(4993,[1742],[1.0])|\n",
      "|  38016|        3|     0|       1028|    0|(8,[3],[1.0])|(4993,[1028],[1.0])|\n",
      "+-------+---------+------+-----------+-----+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encodeData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:25:33.986287Z",
     "start_time": "2020-04-17T01:25:33.818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- age_range: integer (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- merchant_id: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- age_range_vec: vector (nullable = true)\n",
      " |-- merchant_id_vec: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-------+---------+------+-----------+-----+-------------+-------------------+----------------------------------+\n",
      "|user_id|age_range|gender|merchant_id|label|age_range_vec|merchant_id_vec    |features                          |\n",
      "+-------+---------+------+-----------+-----+-------------+-------------------+----------------------------------+\n",
      "|34176  |6        |0     |3906       |0    |(8,[6],[1.0])|(4993,[3906],[1.0])|(5002,[6,3914],[1.0,1.0])         |\n",
      "|34176  |6        |0     |121        |0    |(8,[6],[1.0])|(4993,[121],[1.0]) |(5002,[6,129],[1.0,1.0])          |\n",
      "|34176  |6        |0     |4356       |1    |(8,[6],[1.0])|(4993,[4356],[1.0])|(5002,[6,4364],[1.0,1.0])         |\n",
      "|34176  |6        |0     |2217       |0    |(8,[6],[1.0])|(4993,[2217],[1.0])|(5002,[6,2225],[1.0,1.0])         |\n",
      "|230784 |0        |0     |4818       |0    |(8,[0],[1.0])|(4993,[4818],[1.0])|(5002,[0,4826],[1.0,1.0])         |\n",
      "|362112 |4        |1     |2618       |0    |(8,[4],[1.0])|(4993,[2618],[1.0])|(5002,[4,2626,5001],[1.0,1.0,1.0])|\n",
      "|34944  |5        |0     |2051       |0    |(8,[5],[1.0])|(4993,[2051],[1.0])|(5002,[5,2059],[1.0,1.0])         |\n",
      "|231552 |5        |0     |3828       |1    |(8,[5],[1.0])|(4993,[3828],[1.0])|(5002,[5,3836],[1.0,1.0])         |\n",
      "|231552 |5        |0     |2124       |0    |(8,[5],[1.0])|(4993,[2124],[1.0])|(5002,[5,2132],[1.0,1.0])         |\n",
      "|232320 |4        |1     |1168       |0    |(8,[4],[1.0])|(4993,[1168],[1.0])|(5002,[4,1176,5001],[1.0,1.0,1.0])|\n",
      "+-------+---------+------+-----------+-----+-------------+-------------------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#特征归并到一列\n",
    "from pyspark.ml.feature import VectorAssembler  \n",
    "#一个 导入VerctorAssembler 将多个列合并成向量列的特征转换器,即将表中各列用一个类似list表示，输出预测列为单独一列。\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['age_range_vec',  'merchant_id_vec',  'gender'],outputCol=\"features\")\n",
    "trainset = assembler.transform(encodeData)\n",
    "trainset.printSchema()\n",
    "\n",
    "trainset.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:25:36.036301Z",
     "start_time": "2020-04-17T01:25:35.666Z"
    }
   },
   "outputs": [],
   "source": [
    "#模型调参，类似GridSearchCV\n",
    "import pyspark.ml.classification as cl\n",
    "import pyspark.ml.tuning as tune\n",
    "logistic = cl.LogisticRegression(labelCol='label')\n",
    "\n",
    "grid = tune.ParamGridBuilder().addGrid(logistic.maxIter,[2,10,50]).addGrid(logistic.regParam,[0.01,0.05,0.3]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:25:37.049369Z",
     "start_time": "2020-04-17T01:25:37.243Z"
    }
   },
   "outputs": [],
   "source": [
    "#结果评价器\n",
    "import pyspark.ml.evaluation as ev\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability', \n",
    "    labelCol='label')\n",
    "#estimator 评估器，estimatorParamMaps为网格，evaluator比较性能，numFolds进行几折交叉验证    \n",
    "cv = tune.CrossValidator(estimator=logistic,estimatorParamMaps=grid,evaluator=evaluator,numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:31:32.937360Z",
     "start_time": "2020-04-17T01:25:38.459Z"
    }
   },
   "outputs": [],
   "source": [
    "cvmodel = cv.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:34:21.356368Z",
     "start_time": "2020-04-17T01:34:16.604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6998704463737617\n",
      "0.13252691056217156\n"
     ]
    }
   ],
   "source": [
    "results = cvmodel.transform(trainset)\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderPR'})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:34:22.358472Z",
     "start_time": "2020-04-17T01:34:21.245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'maxIter': 2}, {'regParam': 0.05}], 0.6544724424373507)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最优参数\n",
    "results = [\n",
    "    (\n",
    "         [\n",
    "             {key.name : paramValue}\n",
    "             for key,paramValue in zip(params.keys(),params.values())\n",
    "   \n",
    "         ],metric)\n",
    "for params,metric in zip(cvmodel.getEstimatorParamMaps(),cvmodel.avgMetrics)\n",
    "]\n",
    "sorted(results,key=lambda e1 : e1[1],reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:35:41.414260Z",
     "start_time": "2020-04-17T01:35:39.146Z"
    }
   },
   "outputs": [],
   "source": [
    "#预测\n",
    "spark_test = spark_test.withColumn(\"user_id\", spark_test[\"user_id\"].cast(IntegerType()))\n",
    "spark_test = spark_test.withColumn(\"age_range\", spark_test[\"age_range\"].cast(IntegerType()))\n",
    "spark_test = spark_test.withColumn(\"gender\", spark_test[\"gender\"].cast(IntegerType()))\n",
    "spark_test = spark_test.withColumn(\"merchant_id\", spark_test[\"merchant_id\"].cast(IntegerType()))\n",
    " \n",
    "\n",
    "onehot1 = pipeline.fit(spark_test)\n",
    "encodeData1 = onehot1.transform(spark_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:35:42.620970Z",
     "start_time": "2020-04-17T01:35:42.555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+-----------+-----+--------------+-------------+-------------------+-------------------------+\n",
      "|user_id|age_range|gender|merchant_id|label|predict_result|age_range_vec|merchant_id_vec    |features                 |\n",
      "+-------+---------+------+-----------+-----+--------------+-------------+-------------------+-------------------------+\n",
      "|163968 |0        |0     |4378       |-1.0 |0             |(8,[0],[1.0])|(4995,[4378],[1.0])|(5004,[0,4386],[1.0,1.0])|\n",
      "|163968 |0        |0     |2300       |-1.0 |0             |(8,[0],[1.0])|(4995,[2300],[1.0])|(5004,[0,2308],[1.0,1.0])|\n",
      "|163968 |0        |0     |1551       |-1.0 |0             |(8,[0],[1.0])|(4995,[1551],[1.0])|(5004,[0,1559],[1.0,1.0])|\n",
      "|163968 |0        |0     |4343       |-1.0 |0             |(8,[0],[1.0])|(4995,[4343],[1.0])|(5004,[0,4351],[1.0,1.0])|\n",
      "|163968 |0        |0     |4911       |-1.0 |0             |(8,[0],[1.0])|(4995,[4911],[1.0])|(5004,[0,4919],[1.0,1.0])|\n",
      "|163968 |0        |0     |4043       |-1.0 |0             |(8,[0],[1.0])|(4995,[4043],[1.0])|(5004,[0,4051],[1.0,1.0])|\n",
      "|163968 |0        |0     |2138       |-1.0 |0             |(8,[0],[1.0])|(4995,[2138],[1.0])|(5004,[0,2146],[1.0,1.0])|\n",
      "|163968 |0        |0     |2575       |-1.0 |0             |(8,[0],[1.0])|(4995,[2575],[1.0])|(5004,[0,2583],[1.0,1.0])|\n",
      "|163968 |0        |0     |4159       |-1.0 |0             |(8,[0],[1.0])|(4995,[4159],[1.0])|(5004,[0,4167],[1.0,1.0])|\n",
      "|163968 |0        |0     |1163       |-1.0 |0             |(8,[0],[1.0])|(4995,[1163],[1.0])|(5004,[0,1171],[1.0,1.0])|\n",
      "+-------+---------+------+-----------+-----+--------------+-------------+-------------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler1 = VectorAssembler(inputCols=['age_range_vec',  'merchant_id_vec',  'gender'],outputCol=\"features\")\n",
    "testset = assembler1.transform(encodeData1)\n",
    "\n",
    "testset.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:35:47.485192Z",
     "start_time": "2020-04-17T01:35:44.732Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = cl.LogisticRegression(\n",
    "    maxIter=2, \n",
    "    regParam=0.05, \n",
    "    labelCol='label')\n",
    "lr1 = lr.fit(trainset.select(['features','label']))\n",
    "test_model = lr1.transform(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:36:57.490748Z",
     "start_time": "2020-04-17T01:36:56.788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[user_id: int, age_range: int, gender: int, merchant_id: int, label: double, predict_result: bigint, age_range_vec: vector, merchant_id_vec: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]\n"
     ]
    }
   ],
   "source": [
    "result = test_model.select(\"user_id\",\"prediction\") \n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T01:40:16.300334Z",
     "start_time": "2020-04-17T01:40:15.107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Py4JJavaError: An error occurred while calling o10476.save.\n",
       ": org.apache.spark.SparkException: Job aborted.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
       "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
       "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
       "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1161.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1161.0 (TID 2159, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
       "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n",
       "\t... 10 more\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5004, y.size = 5002\n",
       "\tat scala.Predef$.require(Predef.scala:224)\n",
       "\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:1001)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:1000)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1157)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:930)\n",
       "\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n",
       "\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n",
       "\t... 18 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
       "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
       "\tat scala.Option.foreach(Option.scala:257)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n",
       "\t... 32 more\n",
       "Caused by: org.apache.spark.SparkException: Task failed while writing rows.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
       "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n",
       "\t... 10 more\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5004, y.size = 5002\n",
       "\tat scala.Predef$.require(Predef.scala:224)\n",
       "\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:104)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:1001)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegressionModel$$anonfun$33.apply(LogisticRegression.scala:1000)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:1157)\n",
       "\tat org.apache.spark.ml.classification.LogisticRegressionModel.predictRaw(LogisticRegression.scala:930)\n",
       "\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:117)\n",
       "\tat org.apache.spark.ml.classification.ProbabilisticClassificationModel$$anonfun$1.apply(ProbabilisticClassifier.scala:116)\n",
       "\t... 18 more\n",
       "\n",
       "(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o10476.save.\\n', JavaObject id=o10477), <traceback object at 0x7ff684647140>)\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:282)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.7.7\n"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
