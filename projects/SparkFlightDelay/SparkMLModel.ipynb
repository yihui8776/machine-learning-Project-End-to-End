{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spark/python/pyspark/context.py:123: UserWarning: You are passing in an insecure Py4j gateway.  This presents a security risk, and will be completely forbidden in Spark 3.0\n",
      "  \"You are passing in an insecure Py4j gateway.  This \"\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "from pyspark.sql import SparkSession \n",
    "spark=SparkSession.builder.appName(\"dataframe\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = spark.read.csv(r\"data_mod.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题探索\n",
    "这是航班的历史数据，当然可以做些统计分析看些有何问题，当然我们更重要是做些数据挖掘，这其实可以定位出很多问题来。比如相关分析，看航班和延误的关联关系，研究航班延误的相关因素，还可以做回归，直接把延迟时间作为目标来做回归分析，也可以将是否延误作为一个目标来做二分类预测，也可以将延迟时间分段来作分析。这个就需要从各个角度来看这个延迟时间的问题。  \n",
    "比如从公共交通事业的角度，我们可以看那机场的延误来比较航班的及时率等服务水平，如果一些公共事务的资源调度也可以参考。另外从旅客的角度，我们可以根据大数据选择一些优化的航线，减少碰到延误的概率，从而规划自己的行程。当然更多是航空公司和保险公司的商业角度,这可能还要关联其他数据。例如航班延误险是很多公司都有的服务，但是一般是根据多少时间，30分钟，60分钟或是2个小时等分段赔付多少，或是直接设置一个赔付阈值，比如国内的比较长，2个小时3个小时以上都有。  \n",
    "这里我们做个判断，假设是30分钟为一个分界点，也就是起飞延迟大于30分钟设为1，其它为0，这就是个二分类问题。然后建立分类模型，再用数据预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成目标变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "# 定义一个 udf 函数 \n",
    "def IsDelay(DepDelay):\n",
    "    if DepDelay>30:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# 返回类型为int类型\n",
    "udfDelay = udf(IsDelay, IntegerType())\n",
    "# 使用\n",
    "\n",
    "data1 =  data.withColumn('IsDelay', udfDelay(data.DepDelay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|IsDelay|  count|\n",
      "+-------+-------+\n",
      "|      1|  90945|\n",
      "|      0|1196388|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.groupBy('IsDelay').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+--------+--------+------+----+--------+-------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|ActualElapsedTime|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|IsDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+--------+--------+------+----+--------+-------+\n",
      "|1987|   10|        14|        3|    741|       730|    912|       849|           PS|     1451|               91|            79|      23|      11|   SAN| SFO|   447.0|      0|\n",
      "|1987|   10|        15|        4|    729|       730|    903|       849|           PS|     1451|               94|            79|      14|      -1|   SAN| SFO|   447.0|      0|\n",
      "|1987|   10|        17|        6|    741|       730|    918|       849|           PS|     1451|               97|            79|      29|      11|   SAN| SFO|   447.0|      0|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+--------+--------+------+----+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征选择\n",
    "先根据业务数据个人理解，year一般没多少区分，Month可能反应淡季旺季，也可能和天气有关，可以选择；  \n",
    "DayofMonth 是几月几日，不是明显特征；  \n",
    "DayOfWeek 是星期几，可以反应哪天比较多，是选择的特征；  \n",
    "CRSDepTime、CRSArrTime、CRSElapsedTime是规定时间，可以获取的特征数据；  \n",
    "UniqueCarrier、FlightNum、Origin、Dest 是分类特征；  \n",
    "Distance为数值变量，距离也是最值得参考的特征；  \n",
    "另外DayOfWeek也可能需要one-hot转换。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'ActualElapsedTime', 'CRSElapsedTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', 'Distance', 'IsDelay']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Month',   'DayOfWeek',  'CRSDepTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'CRSElapsedTime', 'Origin', 'Dest', 'Distance']\n",
    "target = 'IsDelay'\n",
    "\n",
    "categoryFeatures = ['UniqueCarrier', 'FlightNum','Origin', 'Dest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征处理\n",
    "首先将标签特征转为数值特征，这里使用StringIndexer,可以设置一个管道来通过转换器传递数据，以便提取特征和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer \n",
    "from pyspark.ml import Pipeline\n",
    "#创建StringIndexer对象，设定输入输出参数\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    StringIndexer(inputCol=c, outputCol='{}_index'.format(c))\n",
    "    for c in categoryFeatures\n",
    "])\n",
    "\n",
    "stringindex = pipeline.fit(data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+--------+--------+------+----+--------+-------+-------------------+---------------+------------+----------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|ActualElapsedTime|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|IsDelay|UniqueCarrier_index|FlightNum_index|Origin_index|Dest_index|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+--------+--------+------+----+--------+-------+-------------------+---------------+------------+----------+\n",
      "|1987|   10|        14|        3|    741|       730|    912|       849|           PS|     1451|               91|            79|      23|      11|   SAN| SFO|   447.0|      0|               11.0|         1081.0|        28.0|       5.0|\n",
      "|1987|   10|        15|        4|    729|       730|    903|       849|           PS|     1451|               94|            79|      14|      -1|   SAN| SFO|   447.0|      0|               11.0|         1081.0|        28.0|       5.0|\n",
      "|1987|   10|        17|        6|    741|       730|    918|       849|           PS|     1451|               97|            79|      29|      11|   SAN| SFO|   447.0|      0|               11.0|         1081.0|        28.0|       5.0|\n",
      "|1987|   10|        18|        7|    729|       730|    847|       849|           PS|     1451|               78|            79|      -2|      -1|   SAN| SFO|   447.0|      0|               11.0|         1081.0|        28.0|       5.0|\n",
      "|1987|   10|        19|        1|    749|       730|    922|       849|           PS|     1451|               93|            79|      33|      19|   SAN| SFO|   447.0|      0|               11.0|         1081.0|        28.0|       5.0|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+--------+--------+------+----+--------+-------+-------------------+---------------+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexData = stringindex.transform(data1)\n",
    "indexData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- ActualElapsedTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- IsDelay: integer (nullable = true)\n",
      " |-- UniqueCarrier_index: double (nullable = false)\n",
      " |-- FlightNum_index: double (nullable = false)\n",
      " |-- Origin_index: double (nullable = false)\n",
      " |-- Dest_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-1b31336a-190d-42fd-82a4-15c313e78824/pyspark_runner.py\", line 158, in <module>\n",
       "    sleep(1)\n",
       "  File \"/spark/python/pyspark/context.py\", line 270, in signal_handler\n",
       "    raise KeyboardInterrupt()\n",
       "KeyboardInterrupt\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:282)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyspark.ml.feature import OneHotEncoder \n",
    "#from pyspark.ml import Pipeline\n",
    "##创建OneHotEncoder对象，设定输入输出参数\n",
    "#categoryFeaturesIndex = ['Month','DayOfWeek','UniqueCarrier_index','FlightNum_index','Origin_index','Dest_index']\n",
    "#pipeline = Pipeline(stages=[\n",
    "#    OneHotEncoder(inputCol=c, outputCol='{}_vec'.format(c))\n",
    "#    for c in [categoryFeaturesIndex]\n",
    "#])\n",
    "\n",
    "#onehot = pipeline.fit(indexData)\n",
    "#encodeData = onehot.transform(indexData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime', 'UniqueCarrier', 'FlightNum', 'ActualElapsedTime', 'CRSElapsedTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest', 'Distance', 'IsDelay', 'UniqueCarrier_index', 'FlightNum_index', 'Origin_index', 'Dest_index']\n",
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- IsDelay: integer (nullable = true)\n",
      " |-- UniqueCarrier_index: double (nullable = false)\n",
      " |-- FlightNum_index: double (nullable = false)\n",
      " |-- Origin_index: double (nullable = false)\n",
      " |-- Dest_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#去除无用特征列\n",
    "print(indexData.columns)\n",
    "use_data = indexData.select([ 'Month',  'DayOfWeek',  'CRSDepTime', 'CRSArrTime', 'CRSElapsedTime',  'Distance', 'IsDelay', 'UniqueCarrier_index', 'FlightNum_index', 'Origin_index', 'Dest_index'])\n",
    "use_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分训练测试集\n",
    "trainData, testData= use_data.randomSplit([0.7, 0.3], seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "901010 386323\n"
     ]
    }
   ],
   "source": [
    "print(trainData.count(),testData.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化\n",
    "spark机器学习的算法和sklearn不同，需要将特征的列组成一个向量输入算法模型里；  \n",
    "这里使用VectorAssembler将给定的列列表组合到单个特征向量列中，就是一个feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- IsDelay: integer (nullable = true)\n",
      " |-- UniqueCarrier_index: double (nullable = false)\n",
      " |-- FlightNum_index: double (nullable = false)\n",
      " |-- Origin_index: double (nullable = false)\n",
      " |-- Dest_index: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---------------------------------------------------+-------+\n",
      "|features                                           |IsDelay|\n",
      "+---------------------------------------------------+-------+\n",
      "|[10.0,1.0,1.0,556.0,235.0,1846.0,1.0,679.0,5.0,0.0]|0      |\n",
      "|[10.0,1.0,1.0,556.0,235.0,1846.0,1.0,679.0,5.0,0.0]|0      |\n",
      "|[10.0,1.0,5.0,35.0,150.0,987.0,10.0,55.0,76.0,19.0]|0      |\n",
      "|[10.0,1.0,5.0,35.0,150.0,987.0,10.0,55.0,76.0,19.0]|0      |\n",
      "|[10.0,1.0,5.0,35.0,150.0,987.0,10.0,55.0,76.0,19.0]|0      |\n",
      "|[10.0,1.0,5.0,35.0,150.0,987.0,10.0,55.0,76.0,19.0]|0      |\n",
      "|[10.0,1.0,5.0,40.0,35.0,142.0,0.0,92.0,21.0,59.0]  |0      |\n",
      "|[10.0,1.0,5.0,40.0,35.0,142.0,0.0,92.0,21.0,59.0]  |0      |\n",
      "|[10.0,1.0,5.0,42.0,37.0,110.0,2.0,59.0,64.0,8.0]   |0      |\n",
      "|[10.0,1.0,5.0,114.0,69.0,337.0,6.0,611.0,3.0,5.0]  |0      |\n",
      "+---------------------------------------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler  #一个 导入VerctorAssembler 将多个列合并成向量列的特征转换器,即将表中各列用一个类似list表示，输出预测列为单独一列。\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[ 'Month',  'DayOfWeek',  'CRSDepTime', 'CRSArrTime', 'CRSElapsedTime',  'Distance', 'UniqueCarrier_index', 'FlightNum_index', 'Origin_index', 'Dest_index'], outputCol=\"features\")\n",
    "trainset = assembler.transform(trainData)\n",
    "trainset.printSchema()\n",
    "\n",
    "trainset.select(['features','IsDelay']).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模\n",
    "使用随机森林建模,随机森林是基于决策树的，决策树是最常见的判别式的建模算法，比如我们看这个飞机延迟问题可以从各个特征开始判断，比如几月，星期几、然后是哪家航空的，哪里飞到哪。用算法可以训练出一个树形的判别方式，最后判别是否延迟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/spark/python/pyspark/sql/utils.py\", line 63, in deco\n",
       "    return f(*a, **kw)\n",
       "  File \"/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
       "    format(target_id, \".\", name), value)\n",
       "py4j.protocol.Py4JJavaError: An error occurred while calling o1596.fit.\n",
       ": java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 50) to be at least as large as the number of values in each categorical feature, but categorical feature 7 has 2161 values. Considering remove this and other categorical features with a large number of values, or add more training examples.\n",
       "\tat scala.Predef$.require(Predef.scala:224)\n",
       "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:137)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)\n",
       "\tat org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:142)\n",
       "\tat org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:120)\n",
       "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
       "\tat scala.util.Try$.apply(Try.scala:192)\n",
       "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
       "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:120)\n",
       "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
       "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-1b31336a-190d-42fd-82a4-15c313e78824/pyspark_runner.py\", line 194, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 3, in <module>\n",
       "  File \"/spark/python/pyspark/ml/base.py\", line 132, in fit\n",
       "    return self._fit(dataset)\n",
       "  File \"/spark/python/pyspark/ml/wrapper.py\", line 295, in _fit\n",
       "    java_model = self._fit_java(dataset)\n",
       "  File \"/spark/python/pyspark/ml/wrapper.py\", line 292, in _fit_java\n",
       "    return self._java_obj.fit(dataset._jdf)\n",
       "  File \"/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
       "    answer, self.gateway_client, self.target_id, self.name)\n",
       "  File \"/spark/python/pyspark/sql/utils.py\", line 79, in deco\n",
       "    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\n",
       "pyspark.sql.utils.IllegalArgumentException: 'requirement failed: DecisionTree requires maxBins (= 50) to be at least as large as the number of values in each categorical feature, but categorical feature 7 has 2161 values. Considering remove this and other categorical features with a large number of values, or add more training examples.'\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:282)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(numTrees=50, labelCol=\"IsDelay\", seed=7  ,maxDepth=8 , maxBins = 50)\n",
    "\n",
    "rfcModel = rfc.fit(trainset.select(['features','IsDelay']))\n",
    "     \n",
    "#输出模型特征重要性、子树权重\n",
    "print(\"模型特征重要性:{}\".format(rfcModel.featureImportances))\n",
    "print(\"模型特征数:{}\".format(rfcModel.numFeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试样本数:386323\n",
      "+-----+---------+----------+----------+--------------+--------+-------+-------------------+---------------+------------+----------+--------------------+\n",
      "|Month|DayOfWeek|CRSDepTime|CRSArrTime|CRSElapsedTime|Distance|IsDelay|UniqueCarrier_index|FlightNum_index|Origin_index|Dest_index|            features|\n",
      "+-----+---------+----------+----------+--------------+--------+-------+-------------------+---------------+------------+----------+--------------------+\n",
      "|   10|        1|         1|       556|           235|  1846.0|      0|                1.0|          679.0|         5.0|       0.0|[10.0,1.0,1.0,556...|\n",
      "|   10|        1|         5|        40|            35|   142.0|      0|                0.0|           92.0|        21.0|      59.0|[10.0,1.0,5.0,40....|\n",
      "|   10|        1|         5|        40|            35|   142.0|      0|                0.0|           92.0|        21.0|      59.0|[10.0,1.0,5.0,40....|\n",
      "|   10|        1|         5|       114|            69|   337.0|      0|                6.0|          611.0|         3.0|       5.0|[10.0,1.0,5.0,114...|\n",
      "|   10|        1|         5|       114|            69|   337.0|      0|                6.0|          611.0|         3.0|       5.0|[10.0,1.0,5.0,114...|\n",
      "|   10|        1|         5|       527|           202|  1593.0|      0|                8.0|          265.0|         3.0|       6.0|[10.0,1.0,5.0,527...|\n",
      "|   10|        1|         5|       758|           293|  2053.0|      0|                3.0|         1226.0|         3.0|      31.0|[10.0,1.0,5.0,758...|\n",
      "|   10|        1|         5|       758|           293|  2053.0|      0|                3.0|         1226.0|         3.0|      31.0|[10.0,1.0,5.0,758...|\n",
      "|   10|        1|         8|       517|           189|  1515.0|      0|                2.0|           14.0|        18.0|       0.0|[10.0,1.0,8.0,517...|\n",
      "|   10|        1|         9|       609|           240|  1781.0|      0|                1.0|          316.0|        52.0|       0.0|[10.0,1.0,9.0,609...|\n",
      "|   10|        1|         9|       609|           240|  1781.0|      0|                1.0|          316.0|        52.0|       0.0|[10.0,1.0,9.0,609...|\n",
      "|   10|        1|        10|        50|            40|   129.0|      0|                1.0|          234.0|        38.0|      22.0|[10.0,1.0,10.0,50...|\n",
      "|   10|        1|        10|        50|            40|   129.0|      0|                1.0|          234.0|        38.0|      22.0|[10.0,1.0,10.0,50...|\n",
      "|   10|        1|        10|        59|            49|   261.0|      0|                0.0|         1195.0|       143.0|      77.0|[10.0,1.0,10.0,59...|\n",
      "|   10|        1|        10|       100|            50|   261.0|      0|               12.0|          364.0|        77.0|     146.0|[10.0,1.0,10.0,10...|\n",
      "|   10|        1|        10|       100|            50|   261.0|      0|               12.0|          364.0|        77.0|     146.0|[10.0,1.0,10.0,10...|\n",
      "|   10|        1|        10|       538|           208|  1464.0|      0|                2.0|          315.0|         5.0|       2.0|[10.0,1.0,10.0,53...|\n",
      "|   10|        1|        14|       534|           200|  1593.0|      0|                8.0|          265.0|         3.0|       6.0|[10.0,1.0,14.0,53...|\n",
      "|   10|        1|        15|        38|            23|    63.0|      0|                0.0|          368.0|       138.0|      78.0|[10.0,1.0,15.0,38...|\n",
      "|   10|        1|        15|       116|            61|   345.0|      0|                6.0|            3.0|        62.0|      19.0|[10.0,1.0,15.0,11...|\n",
      "+-----+---------+----------+----------+--------------+--------+-------+-------------------+---------------+------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-1b31336a-190d-42fd-82a4-15c313e78824/pyspark_runner.py\", line 194, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 4, in <module>\n",
       "NameError: name 'rfcModel' is not defined\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:282)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.测试\n",
    "testset =  assembler.transform(testData)\n",
    " \n",
    "print(\"测试样本数:{}\".format(testset.count()))\n",
    "print(testset.show())\n",
    " \n",
    " \n",
    "result = rfcModel.transform(testset)\n",
    "result.show()\n",
    " \n",
    "#5.分类效果评估\n",
    "total_amount=result.count()\n",
    "correct_amount = result.filter(result.IsDelay==result.prediction).count()\n",
    "precision_rate = correct_amount/total_amount\n",
    "print(\"预测准确率为:{}\".format(precision_rate))\n",
    " \n",
    "positive_precision_amount = result.filter(result.indexed == 0).filter(result.prediction == 0).count()\n",
    "negative_precision_amount = result.filter(result.indexed == 1).filter(result.prediction == 1).count()\n",
    "positive_false_amount = result.filter(result.indexed == 0).filter(result.prediction == 1).count()\n",
    "negative_false_amount = result.filter(result.indexed == 1).filter(result.prediction == 0).count()\n",
    " \n",
    "print(\"正样本预测准确数量:{},负样本预测准确数量:{}\".format(positive_precision_amount,negative_precision_amount))\n",
    " \n",
    "positive_amount = result.filter(result.indexed == 0).count()\n",
    "negative_amount = result.filter(result.indexed == 1).count()\n",
    " \n",
    "print(\"正样本数:{},负样本数:{}\".format(positive_amount,negative_amount))\n",
    "print(\"正样本预测错误数量:{},负样本错误准确数量:{}\".format(positive_false_amount,negative_false_amount))\n",
    " \n",
    "recall_rate1 = positive_precision_amount/positive_amount\n",
    "recall_rate2 = negative_precision_amount/negative_amount\n",
    " \n",
    "print(\"正样本召回率为:{},负样本召回率为:{}\".format(recall_rate1,recall_rate2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.7.7\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
